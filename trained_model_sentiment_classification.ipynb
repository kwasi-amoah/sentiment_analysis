{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574f543-5728-4768-9e17-a447dc86d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Subset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5438982-f471-49ed-a84d-ff85f4fa35ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging to give updates on completed steps\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "#Function to update masterlist\n",
    "def update_masterlist(new_data, master_file='masterlist.csv'):\n",
    "    \"\"\"\n",
    "    Check if a previous masterlist csv file exists, create a new file if it does not or update the master list with new data and save it to a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        new_data (pd.DataFrame): New data to add to the master list.\n",
    "        master_file (str): Path to already-existing master list CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated master list DataFrame.\n",
    "    \"\"\"\n",
    "    if os.path.exists(master_file):\n",
    "        master_df = pd.read_csv(master_file)\n",
    "        logging.info(f\"Loaded existing master list with {len(master_df)} entries.\") #logging updates on completed steps\n",
    "    else:\n",
    "        master_df = pd.DataFrame()\n",
    "        logging.info(\"No master list found. Creating a new one.\")\n",
    "    \n",
    "    #Updating existing masterlist with new data\n",
    "    updated_master = pd.concat([master_df, new_data], ignore_index=True)\n",
    "    updated_master = updated_master.drop_duplicates(subset='tweet_id', keep='last')\n",
    "    updated_master.to_csv(master_file, index=False)\n",
    "    logging.info(f\"Master list updated and saved to {master_file}. Total entries: {len(updated_master)}\") #logging updates on completed steps\n",
    "    return updated_master\n",
    "\n",
    "#Function to fetch tweets\n",
    "def get_recent_tweets_with_context(handle, bearer_token, days_back=6, master_file='masterlist.csv'):\n",
    "    \"\"\"\n",
    "    Retrieve recent tweets\n",
    "    \n",
    "    Args:\n",
    "        handle (str): Twitter handle to fetch tweets for.\n",
    "        bearer_token (str): Twitter API bearer token.\n",
    "        days_back (int): Number of days to look back for tweets.\n",
    "        master_file (str): Path to the master list CSV file.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Updated master list DataFrame with recent tweets.\n",
    "    \"\"\"\n",
    "    client = tweepy.Client(bearer_token=bearer_token, wait_on_rate_limit=True)\n",
    "\n",
    "    #Specify how far back function can retrieve tweets\n",
    "    end_time = datetime.now(timezone.utc).replace(microsecond=0) - timedelta(seconds=30)\n",
    "    start_time = end_time - timedelta(days=days_back)\n",
    "    \n",
    "    start_time_str = start_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_time_str = end_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    \n",
    "    query = f\"@{handle} -is:retweet lang:en\"\n",
    "\n",
    "    #Retrieving posts\n",
    "    try:\n",
    "        paginator = tweepy.Paginator(\n",
    "            client.search_recent_tweets,\n",
    "            query=query,\n",
    "            start_time=start_time_str,\n",
    "            end_time=end_time_str,\n",
    "            max_results=100,\n",
    "            tweet_fields=[\"id\", \"created_at\", \"text\", \"public_metrics\"]\n",
    "        )\n",
    "        \n",
    "        tweets_data = []\n",
    "        for response in paginator:\n",
    "            if response.data:\n",
    "                for tweet in response.data:\n",
    "                    public_metrics = tweet[\"public_metrics\"]\n",
    "                    \n",
    "                    tweets_data.append({\n",
    "                        'tweet_id': tweet.id,\n",
    "                        'date': tweet[\"created_at\"],\n",
    "                        'texts': tweet[\"text\"],\n",
    "                        'likes': public_metrics.get('like_count', 0),\n",
    "                        'retweets': public_metrics.get('retweet_count', 0),\n",
    "                        'replies': public_metrics.get('reply_count', 0),\n",
    "                        'views': public_metrics.get('impression_count', 0),\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(tweets_data)\n",
    "        updated_master_df = update_masterlist(df, master_file)\n",
    "        return updated_master_df\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving tweets: {e}\")\n",
    "        return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "\n",
    "# Replace with your actual Twitter API bearer token\n",
    "BEARER_TOKEN = 'API_KEY'\n",
    "handle = \"@BANK_NAME\"  # Replace with the desired Twitter handle\n",
    "\n",
    "masterlist_df = get_recent_tweets_with_context(handle, BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e33d7c-628b-4917-b383-313a969ab9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "masterlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8572128e-a2d9-4441-947a-a286d99fffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "load_directory = \"Replace with path of model\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_directory)\n",
    "tokenizer = AutoTokenizer.from_pretrained(load_directory)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Load the trainer with the model\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd25a2f-59f4-4af9-8564-4cea77f84312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create teokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"texts\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd045c42-e4e0-4735-9f4d-4b01b25a95b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert masterlist to test dataframe\n",
    "# Ensure test dataset has only text column\n",
    "\n",
    "test_df = masterlist_df.drop(columns = [\"tweet_id\", \"date\", \"likes\", \"retweets\", \"replies\", \"views\"])\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9570a1-e3b9-447d-a7e2-5ad7ba7ce72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test dataset to PyTorch format\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "# Get model predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Convert logits to probabilities\n",
    "probs = softmax(torch.tensor(predictions.predictions), dim=1).numpy()\n",
    "\n",
    "# Add predictions and probabilities to test DataFrame\n",
    "test_df[\"predicted_labels\"] = preds\n",
    "test_df[\"max_probability\"] = np.round(np.max(probs, axis=1), 2)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf80bfe-40da-4bc3-b5c1-fbf430f50e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding predicted labels and probabilities to masterlist\n",
    "# Ensure both DataFrames have the same length\n",
    "\n",
    "if len(masterlist_df) == len(test_df):\n",
    "    masterlist_df[\"predicted_labels\"] = test_df[\"predicted_labels\"].values\n",
    "    masterlist_df[\"max_probability\"] = test_df[\"max_probability\"].values\n",
    "else:\n",
    "    print(\"Error: DataFrames have different lengths!\")\n",
    "\n",
    "#Converting numerical predicted labels back to word labels\n",
    "# Define mapping dictionary\n",
    "label_mapping = {\n",
    "    0: \"Complaint\",\n",
    "    1: \"Enquiry\",\n",
    "    2: \"Other\",\n",
    "    3: \"Praise\",\n",
    "    4: \"Promo\",\n",
    "    5: \"Reaction\",\n",
    "    6: \"Recommendation\",\n",
    "    7: \"Response\"\n",
    "}\n",
    "\n",
    "# Apply mapping to convert numerical labels to word labels\n",
    "masterlist_df[\"word_labels\"] = masterlist_df[\"predicted_labels\"].map(label_mapping)\n",
    "\n",
    "#Filtering for labels with probabilities greater than or equal to 0.50\n",
    "final_df = masterlist_df[masterlist_df[\"max_probability\"] >= 0.50]\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc231cc-b509-4e07-82ef-5df4dd94fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop numerical label and keep word label\n",
    "final_df.drop(columns = 'predicted_labels', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064be04f-2d9e-4135-886b-3e0824ccb7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f5b90a-434e-47ed-9cb0-0181d63b1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged DataFrame\n",
    "final_df.to_csv(\"sentiments.csv\", index=False, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b6ca1-e6bd-4cf5-8ced-6845d576233c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
